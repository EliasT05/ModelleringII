{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avsluttende programmerings prosjekt ProgMod\n",
    "## Problemstillingen\n",
    "* Hvordan kan vi bruke programmering til å modellere og simulere kunstig intelligens? \n",
    "* Hvilke faktorer påvirker læring, beslutningstaking og etikk? \n",
    "* Hvilke begrensninger har modellene dine, og drøft hva de forteller oss om kunstig intelligens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduksjon til kunstig inteligens\n",
    "Oxford ordboken definerer kunstig intelligens som “utviklingen av datasystemer i stand til å utføre oppgaver som normalt krever menneskelig intelligens, for eksempel visuell persepsjon, talegjenkjenning, beslutningstaking og oversettelse mellom språk”. Men flere av disse vanlige algoritmene og programmene klarer også å utføre disse oppgavene. Så hva er det som skiller kunstig intelligens fra vanlige datasystemer? Den store forskjellen mellom kunstig intelligens og vanlige programmer er at i vanlig programmering må vi finne mønstrene selv og programmere et system som bruker mønsteret. Med kunstig intelligens skjer utviklingen på en grunnleggende annen måte. Istedenfor å fortelle programmet nøyaktig hva det skal gjøre, gir vi det et datasett, og programmet finner mønstre selv. Ofte kan dette være mer effektivt enn den “gamle måten”, men det kan også ha ulemper, for eksempel ved å finne mønstre som ikke er relevante. Det kan også være partisk hvis datasettet ikke er representativt, og mønstrene AIen finner er derfor ikke representative. Et eksempel på dette er Amazons AI som hjalp dem med ansettelse. Den var partisk fordi den var basert på tidligere CV-er de hadde mottatt som da var mannsdominerte. Programmet anbefalte derfor menn oftere enn kvinner.<sup>[1]</sup> Kunstig intelligens er ikke en perfekt løsning på alt, og man må være klar over hvordan man bruker det på en ansvarlig måte. Men det betyr ikke at det ikke kan være nyttig hvis man forstår begrensningene og bygger rundt dem.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruk av kunstig inteligens\n",
    "Det er enkelt å komme i gang med å bruke KI i dag. Et eksempel er Tensoflow. TensorFlow er et populært rammeverk for maskinlæring og dyp læring som kan brukes til å bygge og trene modeller for ulike formål. For eksempel kan du bruke TensorFlow til å bygge en modell som kan klassifisere bilder av hunder og katter. TensorFlow har også mange ressurser og dokumentasjon tilgjengelig på nettet som kan hjelpe deg med å komme i gang. Under kan du se et eksempel som bruker Tensoflow på et enkelt datasett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2013-04-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m     15\u001b[0m train_data, test_data \u001b[39m=\u001b[39m train_test_split(data, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6317\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m   6318\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m   6319\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[0;32m   6320\u001b[0m     ]\n\u001b[0;32m   6322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6323\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6324\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   6325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6327\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    449\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m    452\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    454\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    455\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    456\u001b[0m     using_cow\u001b[39m=\u001b[39;49musing_copy_on_write(),\n\u001b[0;32m    457\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    353\u001b[0m     result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    355\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(result_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[1;32m--> 511\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m    513\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    515\u001b[0m refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\dtypes\\astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    239\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[0;32m    241\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    244\u001b[0m     \u001b[39m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\dtypes\\astype.py:187\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    184\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     values \u001b[39m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    189\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\dtypes\\astype.py:138\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mor\u001b[39;00m is_object_dtype(arr\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m    137\u001b[0m     \u001b[39m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '2013-04-01'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv')\n",
    "\n",
    "\n",
    "data = data.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Define the features and labels\n",
    "features = train_data.drop('mean(air_temperature P1D)', axis=1)\n",
    "labels = train_data['mean(air_temperature P1D)']\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=[len(features.keys())]),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(features, labels, epochs=10)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_features = test_data.drop('mean(air_temperature P1D)', axis=1)\n",
    "test_labels = test_data['mean(air_temperature P1D)']\n",
    "test_loss, test_acc = model.evaluate(test_features, test_labels)\n",
    "\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv\")\n",
    "df.drop(\"index\", inplace=True, axis=1)\n",
    "def convert_to_unix(df, column):\n",
    "  ref_time = []\n",
    "  #df[column] = pd.to_datetime(df[column])\n",
    "  for index, row in df.iterrows():\n",
    "    ref_time.append(time.mktime(datetime.datetime.strptime(row[\"referenceTime\"], \"%Y-%m-%d\").timetuple()))\n",
    "  df[\"referenceTime\"] = ref_time\n",
    "  return df\n",
    "  \n",
    "df = convert_to_unix(df, \"referenceTime\")\n",
    "print(df)\n",
    "#print(df.head())\n",
    "\n",
    "df_features = df.copy()\n",
    "df_labels = df_features.pop('referenceTime')\n",
    "df_features = np.array(df)\n",
    "df_features\n",
    "\n",
    "normalize = layers.Normalization()\n",
    "normalize.adapt(df_features)\n",
    "\n",
    "norm_vaer_model = tf.keras.Sequential([\n",
    "  normalize,\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "norm_vaer_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                           optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "norm_vaer_model.fit(df_features, df_labels, epochs=10)\n",
    "\n",
    "'''\n",
    "vaer_model = tf.keras.Sequential([\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "vaer_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "vaer_model.fit(df_features, df_labels, epochs=10)\n",
    "\n",
    "'''\n",
    "'''\n",
    "#(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Load the data from the CSV file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv\u001b[39m\u001b[39m'\u001b[39m, parse_dates\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mreferenceTime\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m df\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m \u001b[39m#data['sum(duration_of_sunshine P1D)'] = data['sum(duration_of_sunshine P1D)'].replace({'nan': '0'}, regex=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5119\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5120\u001b[0m     labels: IndexLabel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5127\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5128\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5130\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5264\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5265\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5267\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5268\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5269\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5270\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5271\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5272\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5273\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5274\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6696\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6694\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6695\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6696\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6697\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6698\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['index'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv', parse_dates=['referenceTime'])\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "#data['sum(duration_of_sunshine P1D)'] = data['sum(duration_of_sunshine P1D)'].replace({'nan': '0'}, regex=True)\n",
    "data.fillna(0, inplace=True)\n",
    "print(data)\n",
    "# Convert the referenceTime column to a numerical value\n",
    "data['referenceTime'] = data['referenceTime'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the data\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "data_normalized = scaler.transform(data)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "data = pd.DataFrame(data_normalized, columns=data.columns)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "#df_features = df.copy()\n",
    "#df_labels = df_features.pop('referenceTime')\n",
    "\n",
    "X = data[[\"referenceTime\"]]\n",
    "y = data[['mean(air_temperature P1D)',\"mean(air_temperature P1D)\", \"mean(cloud_area_fraction P1D)\"]]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Create a TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=[X.shape[1]]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# Make predictions using the trained model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(new_data)\n\u001b[1;32m---> 17\u001b[0m \u001b[39mprint\u001b[39m(scaler\u001b[39m.\u001b[39;49minverse_transform(pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mmean(air_temperature P1D)\u001b[39;49m\u001b[39m'\u001b[39;49m:predictions[:, \u001b[39m0\u001b[39;49m]})))\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(predictions[:, \u001b[39m1\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(predictions[:, \u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:1052\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_std:\n\u001b[1;32m-> 1052\u001b[0m         X \u001b[39m*\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_\n\u001b[0;32m   1053\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n\u001b[0;32m   1054\u001b[0m         X \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,5)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new data\n",
    "#new_data = pd.read_csv(\"https://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv\", parse_dates=['referenceTime'])\n",
    "new_data = pd.DataFrame({\n",
    "    'referenceTime': ['2023-01-01']})\n",
    "#new_data.drop(\"index\", inplace=True, axis=1)\n",
    "#new_data = \"2023-01-01\"\n",
    "\n",
    "# Convert the referenceTime column to a numerical value\n",
    "new_data['referenceTime'] = (pd.to_datetime(data['referenceTime']).apply(lambda x: x.timestamp()))\n",
    "print(type(new_data))\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "print(scaler.inverse_transform(pd.DataFrame({'mean(air_temperature P1D)':predictions[:, 0]})))\n",
    "print(predictions[:, 1])\n",
    "print(predictions[:, 2])\n",
    "\n",
    "#print(data)\n",
    "# Add the predictions to the new data\n",
    "#new_data = predictions\n",
    "#print((new_data))\n",
    "#print(scaler.inverse_transform(new_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kunstig inteligens i dag\n",
    "Kunstig intelligens (KI) har blitt stadig mer utbredt i dagens samfunn og har mange bruksområder, inkludert chat-modeller. Chat-modeller er en type KI som brukes til å simulere en samtale mellom mennesker og maskiner. Disse modellene kan brukes til å automatisere kundeservice og gi raskere svar på spørsmål fra kunder.\n",
    "\n",
    "Det er imidlertid også bekymringer knyttet til bruken av KI i chat-modeller og andre applikasjoner. For eksempel kan KI-systemer være sårbare for angrep og manipulasjon, og det kan være vanskelig å avgjøre om en samtale skjer mellom et menneske eller en maskin. Det er også bekymringer knyttet til personvern og sikkerhet når det gjelder innsamling og bruk av data i KI-systemer.\n",
    "\n",
    "Når det gjelder ChatGPT, er dette en modell som ble utviklet av OpenAI for å simulere samtaler mellom mennesker og maskiner. Microsoft integrerte denne modellen i Bing for å gi brukerne bedre søkemotorer og anbefalingssystemer. Selv om ChatGPT har gjort det mulig for den gjennomsnittlige forbrukeren å bruke AI i det daglige livet sitt, har det også vært noen utfordringer med modellen. For eksempel var GPT-3-modellen som ChatGPT var basert på, trent på eldre data og var optimalisert for samtaleflyt og ikke nøyaktighet. Dette førte til at modellen slet med matematikk og fant opp fakta. Microsoft integrerte senere OpenAI sin GPT-3-modell i Bing for å gi brukerne bedre søkemotorer og anbefalingssystemer.\n",
    "\n",
    "\n",
    "En bekymring som har kommet frem etter at chat-modeller ble populære er at forbrukere kan ha urealistiske forventninger til hva KI kan gjøre. For eksempel kan noen forbrukere tro at KI-systemer kan løse alle problemer eller gi nøyaktige svar på alle spørsmål. Dette kan føre til at forbrukere tar beslutninger basert på feilaktig informasjon eller at de blir skuffet når KI-systemer ikke fungerer som forventet. Chat-modeller er er ofte veldig selvsikre selv når de har feil, noe som kan føre til at forburkere bil villedet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fotnoter\n",
    "<sup id=“fn1”>[1]</sup> The Guardian\n",
    "\n",
    "<span id=\"fn2\"> footnote 1</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

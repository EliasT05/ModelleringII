{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avsluttende programmerings prosjekt ProgMod\n",
    "## Problemstillingen\n",
    "* Hvordan kan vi bruke programmering til å modellere og simulere kunstig intelligens? \n",
    "* Hvilke faktorer påvirker læring, beslutningstaking og etikk? \n",
    "* Hvilke begrensninger har modellene dine, og drøft hva de forteller oss om kunstig intelligens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduksjon til kunstig inteligens\n",
    "Oxford ordboken definerer kunstig intelligens som “utviklingen av datasystemer i stand til å utføre oppgaver som normalt krever menneskelig intelligens, for eksempel visuell persepsjon, talegjenkjenning, beslutningstaking og oversettelse mellom språk”. Men flere av disse vanlige algoritmene og programmene klarer også å utføre disse oppgavene. Så hva er det som skiller kunstig intelligens fra vanlige datasystemer? Den store forskjellen mellom kunstig intelligens og vanlige programmer er at i vanlig programmering må vi finne mønstrene selv og programmere et system som bruker mønsteret. Med kunstig intelligens skjer utviklingen på en grunnleggende annen måte. Istedenfor å fortelle programmet nøyaktig hva det skal gjøre, gir vi det et datasett, og programmet finner mønstre selv. Ofte kan dette være mer effektivt enn den “gamle måten”, men det kan også ha ulemper, for eksempel ved å finne mønstre som ikke er relevante. Det kan også være partisk hvis datasettet ikke er representativt, og mønstrene AIen finner er derfor ikke representative. Et eksempel på dette er Amazons AI som hjalp dem med ansettelse. Den var partisk fordi den var basert på tidligere CV-er de hadde mottatt som da var mannsdominerte. Programmet anbefalte derfor menn oftere enn kvinner.<sup>[1]</sup> Kunstig intelligens er ikke en perfekt løsning på alt, og man må være klar over hvordan man bruker det på en ansvarlig måte. Men det betyr ikke at det ikke kan være nyttig hvis man forstår begrensningene og bygger rundt dem.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruk av kunstig inteligens\n",
    "Det er enkelt å komme i gang med å bruke KI i dag. Et eksempel er Tensoflow. TensorFlow er et populært rammeverk for maskinlæring og dyp læring som kan brukes til å bygge og trene modeller for ulike formål. For eksempel kan du bruke TensorFlow til å bygge en modell som kan klassifisere bilder av hunder og katter. TensorFlow har også mange ressurser og dokumentasjon tilgjengelig på nettet som kan hjelpe deg med å komme i gang. Under kan du se et eksempel som bruker Tensoflow på et enkelt datasett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def map_string_to_number(s):\n",
    "    # Define your custom mapping from strings to numbers here\n",
    "    if s == 'Adelie' or s == 'MALE':\n",
    "        return float(0)\n",
    "    elif s == 'Chinstrap' or s == 'FEMALE':\n",
    "        return float(1)\n",
    "    elif s == 'Gentoo':\n",
    "        return float(2)\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "\n",
    "#print(iris)\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#index,referenceTime,mean(air_temperature P1D),mean(cloud_area_fraction P1D),sum(duration_of_sunshine P1D)\n",
    "\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('C:\\\\Users\\\\elias\\\\Downloads\\\\vaerdata13-22.csv')#, parse_dates=['referenceTime'])\n",
    "data = data.iloc[: , 1:]\n",
    "data = data.iloc[: , 2:]\n",
    "\n",
    "#data['sum(duration_of_sunshine P1D)'] = data['sum(duration_of_sunshine P1D)'].replace({'nan': '0'}, regex=True)\n",
    "data.fillna(0, inplace=True)\n",
    "print(data)\n",
    "# Convert the referenceTime column to a numerical value\n",
    "data['referenceTime'] = data['referenceTime'].apply(lambda x: x.timestamp())\n",
    "\n",
    "#data = data.T\n",
    "print(data)\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the data\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "data_normalized = scaler.transform(data)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "data = pd.DataFrame(data_normalized, columns=data.columns)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "#df_features = df.copy()\n",
    "#df_labels = df_features.pop('referenceTime')\n",
    "\n",
    "X = data[[\"referenceTime\"]]\n",
    "y = data[['mean(air_temperature P1D)',\"mean(air_temperature P1D)\", \"mean(cloud_area_fraction P1D)\"]]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Create a TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=[X.shape[1]]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "treningsandel = 0.8 # Velger 80 prosent av datasettet til trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "pingvindata = pd.read_csv(\"C:\\\\Users\\\\elias\\\\Documents\\\\GitHub_loc\\\\ModelleringII\\\\penguings.txt\", delimiter = \",\")\n",
    "\n",
    "kriterier = pingvindata[['bill_length_mm', 'bill_depth_mm']] # features\n",
    "kategorier = pingvindata['species']                          # labels\n",
    "\n",
    "# Velge ut data til trening og testing\n",
    "\n",
    "ml_data = train_test_split(kriterier, kategorier, train_size=treningsandel, random_state=42)\n",
    "\n",
    "treningskriterier = ml_data[0]\n",
    "testkriterier = ml_data[1]\n",
    "treningskategorier = ml_data[2]\n",
    "testkategorier = ml_data[3]\n",
    "\n",
    "#print(treningskriterier)\n",
    "\n",
    "modell = tree.DecisionTreeClassifier()                  # Lager modellen\n",
    "modell.fit(treningskriterier, treningskategorier)       # Trener modellen\n",
    "\n",
    "forutsigelser = modell.predict(testkriterier)\n",
    "print(\"Accuracy:\", round(accuracy_score(testkategorier, forutsigelser), 2))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume y_test and y_pred are the true and predicted labels for the test set\n",
    "precision = precision_score(testkategorier, forutsigelser, average='macro')\n",
    "recall = recall_score(testkategorier, forutsigelser, average='macro')\n",
    "f1 = f1_score(testkategorier, forutsigelser, average='macro')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')  \n",
    "print(f'F1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "pingvindata = pd.read_csv(\"C:\\\\Users\\\\elias\\\\Documents\\\\GitHub_loc\\\\ModelleringII\\\\penguings.txt\", delimiter = \",\")\n",
    "\n",
    "kriterier = pingvindata[['bill_length_mm', 'bill_depth_mm']] # features\n",
    "kategorier = pingvindata['species']                          # labels\n",
    "\n",
    "# Velge ut data til trening og testing\n",
    "\n",
    "ml_data = train_test_split(kriterier, kategorier, train_size=treningsandel, random_state=42)\n",
    "\n",
    "treningskriterier = ml_data[0]\n",
    "testkriterier = ml_data[1]\n",
    "treningskategorier = ml_data[2]\n",
    "testkategorier = ml_data[3]\n",
    "\n",
    "#print(treningskriterier)\n",
    "\n",
    "modell = RandomForestClassifier()                  # Lager modellen\n",
    "modell.fit(treningskriterier, treningskategorier)       # Trener modellen\n",
    "\n",
    "forutsigelser = modell.predict(testkriterier)\n",
    "print(\"Accuracy:\", round(accuracy_score(testkategorier, forutsigelser), 2))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume y_test and y_pred are the true and predicted labels for the test set\n",
    "precision = precision_score(testkategorier, forutsigelser, average='macro')\n",
    "recall = recall_score(testkategorier, forutsigelser, average='macro')\n",
    "f1 = f1_score(testkategorier, forutsigelser, average='macro')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')  \n",
    "print(f'F1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "pingvindata = pd.read_csv(\"C:\\\\Users\\\\elias\\\\Documents\\\\GitHub_loc\\\\ModelleringII\\\\penguings.txt\", delimiter = \",\")\n",
    "\n",
    "kriterier = pingvindata[['bill_length_mm', 'bill_depth_mm']] # features\n",
    "kategorier = pingvindata['species']                          # labels\n",
    "\n",
    "# Velge ut data til trening og testing\n",
    "ml_data = train_test_split(kriterier, kategorier, train_size=treningsandel, random_state=42)\n",
    "\n",
    "treningskriterier = ml_data[0]\n",
    "testkriterier = ml_data[1]\n",
    "treningskategorier = ml_data[2]\n",
    "testkategorier = ml_data[3]\n",
    "\n",
    "#print(treningskriterier)\n",
    "\n",
    "modell = LogisticRegression()                  # Lager modellen\n",
    "modell.fit(treningskriterier, treningskategorier)       # Trener modellen\n",
    "\n",
    "forutsigelser = modell.predict(testkriterier)\n",
    "print(\"Accuracy:\", round(accuracy_score(testkategorier, forutsigelser), 2))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume y_test and y_pred are the true and predicted labels for the test set\n",
    "precision = precision_score(testkategorier, forutsigelser, average='macro')\n",
    "recall = recall_score(testkategorier, forutsigelser, average='macro')\n",
    "f1 = f1_score(testkategorier, forutsigelser, average='macro')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')  \n",
    "print(f'F1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "pingvindata = pd.read_csv(\"C:\\\\Users\\\\elias\\\\Documents\\\\GitHub_loc\\\\ModelleringII\\\\penguings.txt\", delimiter = \",\")\n",
    "\n",
    "kriterier = pingvindata[['bill_length_mm', 'bill_depth_mm']] # features\n",
    "kategorier = pingvindata['species']                          # labels\n",
    "\n",
    "# Velge ut data til trening og testing\n",
    "ml_data = train_test_split(kriterier, kategorier, train_size=treningsandel, random_state=42)\n",
    "\n",
    "treningskriterier = ml_data[0]\n",
    "testkriterier = ml_data[1]\n",
    "treningskategorier = ml_data[2]\n",
    "testkategorier = ml_data[3]\n",
    "\n",
    "#print(treningskriterier)\n",
    "\n",
    "modell = KNeighborsClassifier()                  # Lager modellen\n",
    "modell.fit(treningskriterier, treningskategorier)       # Trener modellen\n",
    "\n",
    "forutsigelser = modell.predict(testkriterier)\n",
    "print(\"Accuracy:\", round(accuracy_score(testkategorier, forutsigelser), 2))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume y_test and y_pred are the true and predicted labels for the test set\n",
    "precision = precision_score(testkategorier, forutsigelser, average='macro')\n",
    "recall = recall_score(testkategorier, forutsigelser, average='macro')\n",
    "f1 = f1_score(testkategorier, forutsigelser, average='macro')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')  \n",
    "print(f'F1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "pingvindata = pd.read_csv(\"C:\\\\Users\\\\elias\\\\Documents\\\\GitHub_loc\\\\ModelleringII\\\\penguings.txt\", delimiter = \",\")\n",
    "\n",
    "kriterier = pingvindata[['bill_length_mm', 'bill_depth_mm']] # features\n",
    "kategorier = pingvindata['species']                          # labels\n",
    "\n",
    "# Velge ut data til trening og testing\n",
    "ml_data = train_test_split(kriterier, kategorier, train_size=treningsandel, random_state=42)\n",
    "\n",
    "treningskriterier = ml_data[0]\n",
    "testkriterier = ml_data[1]\n",
    "treningskategorier = ml_data[2]\n",
    "testkategorier = ml_data[3]\n",
    "\n",
    "#print(treningskriterier)\n",
    "\n",
    "modell = SVC()                  # Lager modellen\n",
    "modell.fit(treningskriterier, treningskategorier)       # Trener modellen\n",
    "\n",
    "forutsigelser = modell.predict(testkriterier)\n",
    "print(\"Accuracy:\", round(accuracy_score(testkategorier, forutsigelser), 2))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume y_test and y_pred are the true and predicted labels for the test set\n",
    "precision = precision_score(testkategorier, forutsigelser, average='macro')\n",
    "recall = recall_score(testkategorier, forutsigelser, average='macro')\n",
    "f1 = f1_score(testkategorier, forutsigelser, average='macro')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')  \n",
    "print(f'F1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "pingvindata = pd.read_csv(\"C:\\\\Users\\\\elias\\\\Documents\\\\GitHub_loc\\\\ModelleringII\\\\penguings.txt\", delimiter = \",\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(10, activation=\"sigmoid\", input_shape=(2,)))\n",
    "model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "kriterier = pingvindata[['bill_length_mm', 'bill_depth_mm']] # features\n",
    "kategorier = pingvindata['species']  \n",
    "ml_data = train_test_split(kriterier, kategorier, train_size=treningsandel, random_state=42)\n",
    "x = ml_data[0]\n",
    "X_test = ml_data[1]\n",
    "y = ml_data[2]\n",
    "y_test = ml_data[3]\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return tf.math.reduce_mean((y-y_pred)**2)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.1),\n",
    "              loss=loss)\n",
    "\n",
    "model.fit(x, y, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.68\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.62\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.65\n",
      "Accuracy: 0.65\n",
      "Accuracy: 0.68\n",
      "Accuracy: 0.68\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.65\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.68\n",
      "Accuracy: 0.17\n",
      "Accuracy: 0.68\n",
      "Accuracy: 0.17\n",
      "Average accuracy: 0.395\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "pingvindata = pd.read_csv(\"C:\\\\Users\\\\elias\\\\Documents\\\\GitHub_loc\\\\ModelleringII\\\\penguings.txt\", delimiter = \",\")\n",
    "\n",
    "pingvindata['species'] = pingvindata['species'].map(map_string_to_number)\n",
    "pingvindata['sex'] = pingvindata['sex'].map(map_string_to_number)\n",
    "\n",
    "accuracy_list=[]\n",
    "\n",
    "\n",
    "\n",
    "kriterier = pingvindata[['bill_length_mm', 'bill_depth_mm']] # features\n",
    "kategorier = pingvindata['species']\n",
    "for i in range(20):  \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(10, activation=\"sigmoid\", input_shape=(2,)))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    ml_data = train_test_split(kriterier, kategorier, train_size=treningsandel, random_state=42)\n",
    "    x = ml_data[0]\n",
    "    X_test = ml_data[1]\n",
    "    y = ml_data[2]\n",
    "    y_test = ml_data[3]\n",
    "\n",
    "    def loss(y, y_pred):\n",
    "        return float(tf.math.reduce_mean((y-y_pred)**2))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.1),\n",
    "                loss=loss,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x, y, epochs=1000, verbose=0)\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print('Accuracy:', round(accuracy, 2))\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "print('Average accuracy:', round(sum(accuracy_list)/len(accuracy_list),3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kunstig inteligens i dag\n",
    "Kunstig intelligens (KI) har blitt stadig mer utbredt i dagens samfunn og har mange bruksområder, inkludert chat-modeller. Chat-modeller er en type KI som brukes til å simulere en samtale mellom mennesker og maskiner. Disse modellene kan brukes til å automatisere kundeservice og gi raskere svar på spørsmål fra kunder.\n",
    "\n",
    "Det er imidlertid også bekymringer knyttet til bruken av KI i chat-modeller og andre applikasjoner. For eksempel kan KI-systemer være sårbare for angrep og manipulasjon, og det kan være vanskelig å avgjøre om en samtale skjer mellom et menneske eller en maskin. Det er også bekymringer knyttet til personvern og sikkerhet når det gjelder innsamling og bruk av data i KI-systemer.\n",
    "\n",
    "Når det gjelder ChatGPT, er dette en modell som ble utviklet av OpenAI for å simulere samtaler mellom mennesker og maskiner. Microsoft integrerte denne modellen i Bing for å gi brukerne bedre søkemotorer og anbefalingssystemer. Selv om ChatGPT har gjort det mulig for den gjennomsnittlige forbrukeren å bruke AI i det daglige livet sitt, har det også vært noen utfordringer med modellen. For eksempel var GPT-3-modellen som ChatGPT var basert på, trent på eldre data og var optimalisert for samtaleflyt og ikke nøyaktighet. Dette førte til at modellen slet med matematikk og fant opp fakta. Microsoft integrerte senere OpenAI sin GPT-3-modell i Bing for å gi brukerne bedre søkemotorer og anbefalingssystemer.\n",
    "\n",
    "\n",
    "En bekymring som har kommet frem etter at chat-modeller ble populære er at forbrukere kan ha urealistiske forventninger til hva KI kan gjøre. For eksempel kan noen forbrukere tro at KI-systemer kan løse alle problemer eller gi nøyaktige svar på alle spørsmål. Dette kan føre til at forbrukere tar beslutninger basert på feilaktig informasjon eller at de blir skuffet når KI-systemer ikke fungerer som forventet. Chat-modeller er er ofte veldig selvsikre selv når de har feil, noe som kan føre til at forburkere bil villedet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fotnoter\n",
    "<sup id=“fn1”>[1]</sup> The Guardian\n",
    "\n",
    "<span id=\"fn2\"> footnote 1</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avsluttende programmerings prosjekt ProgMod\n",
    "## Problemstillingen\n",
    "* Hvordan kan vi bruke programmering til å modellere og simulere kunstig intelligens? \n",
    "* Hvilke faktorer påvirker læring, beslutningstaking og etikk? \n",
    "* Hvilke begrensninger har modellene dine, og drøft hva de forteller oss om kunstig intelligens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hva er kunstig inteligens?\n",
    "Oxford orboken definerer kunstig inteligens som; \"utviklingen av datasystemer i stand til å utføre oppgaver som normalt krever menneskelig intelligens, for eksempel visuell persepsjon, talegjenkjenning, beslutningstaking og oversettelse mellom språk\", men flere av disse vanlige algoritmer og programmer klarer å gjøre så hva er det som skiller kunstig inteligens fra vanlig datasystemer. Den store forskjellen på kunstig inteligens og vanlige programmer er at i vanlige programmering må vi finne mønstere selv og programmer et system som bruker mønsteret. Med kunstig inteligens skjer utviklingen på en grunnleggende annen måte. Istedenfor å fortelle programmet nøyaktig hva det skal gjøre gir vi det et datasett, og programmet finner mønstre selv. Ofte kan dette være mer effektivt enn den \"gamle måten\", men det kan også ha ulemper, f.eks. ved å finne mønstre som ikke er relevante. Det kan også være partisk om datasettet ikke er representativt og mønsterene AIen finner er derfor ikke representative. Et eksempel på dette er Amazon sin AI som hjalp dem med ansettelse, den var partisk fordi den var basert på tidligere CVer de hadde mottatt som da var mannsdominerte, programmet anbefalte derfor menn oftere enn kvinner.<sup>[[1]](#fn1)</sup> Kunstig inteligens er ikke perfekt løsning på alt og man må være klar over hvordan man bruker det på en ansvarlig måte, men det betyr ikke at det ikke kan være nyttig om man forstår begrensningene og bygger rundt dem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv\")\n",
    "df.drop(\"index\", inplace=True, axis=1)\n",
    "def convert_to_unix(df, column):\n",
    "  ref_time = []\n",
    "  #df[column] = pd.to_datetime(df[column])\n",
    "  for index, row in df.iterrows():\n",
    "    ref_time.append(time.mktime(datetime.datetime.strptime(row[\"referenceTime\"], \"%Y-%m-%d\").timetuple()))\n",
    "  df[\"referenceTime\"] = ref_time\n",
    "  return df\n",
    "  \n",
    "df = convert_to_unix(df, \"referenceTime\")\n",
    "print(df)\n",
    "#print(df.head())\n",
    "\n",
    "df_features = df.copy()\n",
    "df_labels = df_features.pop('referenceTime')\n",
    "df_features = np.array(df)\n",
    "df_features\n",
    "\n",
    "normalize = layers.Normalization()\n",
    "normalize.adapt(df_features)\n",
    "\n",
    "norm_vaer_model = tf.keras.Sequential([\n",
    "  normalize,\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "norm_vaer_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                           optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "norm_vaer_model.fit(df_features, df_labels, epochs=10)\n",
    "\n",
    "'''\n",
    "vaer_model = tf.keras.Sequential([\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "vaer_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "vaer_model.fit(df_features, df_labels, epochs=10)\n",
    "\n",
    "'''\n",
    "'''\n",
    "#(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Load the data from the CSV file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv\u001b[39m\u001b[39m'\u001b[39m, parse_dates\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mreferenceTime\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m df\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m \u001b[39m#data['sum(duration_of_sunshine P1D)'] = data['sum(duration_of_sunshine P1D)'].replace({'nan': '0'}, regex=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5119\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5120\u001b[0m     labels: IndexLabel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5127\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5128\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5130\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5264\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5265\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5267\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5268\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5269\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5270\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5271\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5272\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5273\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5274\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6696\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6694\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6695\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6696\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6697\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6698\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['index'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv', parse_dates=['referenceTime'])\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "#data['sum(duration_of_sunshine P1D)'] = data['sum(duration_of_sunshine P1D)'].replace({'nan': '0'}, regex=True)\n",
    "data.fillna(0, inplace=True)\n",
    "print(data)\n",
    "# Convert the referenceTime column to a numerical value\n",
    "data['referenceTime'] = data['referenceTime'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the data\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "data_normalized = scaler.transform(data)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "data = pd.DataFrame(data_normalized, columns=data.columns)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "#df_features = df.copy()\n",
    "#df_labels = df_features.pop('referenceTime')\n",
    "\n",
    "X = data[[\"referenceTime\"]]\n",
    "y = data[['mean(air_temperature P1D)',\"mean(air_temperature P1D)\", \"mean(cloud_area_fraction P1D)\"]]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Create a TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=[X.shape[1]]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# Make predictions using the trained model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(new_data)\n\u001b[1;32m---> 17\u001b[0m \u001b[39mprint\u001b[39m(scaler\u001b[39m.\u001b[39;49minverse_transform(pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mmean(air_temperature P1D)\u001b[39;49m\u001b[39m'\u001b[39;49m:predictions[:, \u001b[39m0\u001b[39;49m]})))\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(predictions[:, \u001b[39m1\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(predictions[:, \u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:1052\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_std:\n\u001b[1;32m-> 1052\u001b[0m         X \u001b[39m*\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_\n\u001b[0;32m   1053\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n\u001b[0;32m   1054\u001b[0m         X \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,5)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new data\n",
    "#new_data = pd.read_csv(\"https://raw.githubusercontent.com/EliasT05/vaerdata/main/vaerdata13-22.csv\", parse_dates=['referenceTime'])\n",
    "new_data = pd.DataFrame({\n",
    "    'referenceTime': ['2023-01-01']})\n",
    "#new_data.drop(\"index\", inplace=True, axis=1)\n",
    "#new_data = \"2023-01-01\"\n",
    "\n",
    "# Convert the referenceTime column to a numerical value\n",
    "new_data['referenceTime'] = (pd.to_datetime(data['referenceTime']).apply(lambda x: x.timestamp()))\n",
    "print(type(new_data))\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "print(scaler.inverse_transform(pd.DataFrame({'mean(air_temperature P1D)':predictions[:, 0]})))\n",
    "print(predictions[:, 1])\n",
    "print(predictions[:, 2])\n",
    "\n",
    "#print(data)\n",
    "# Add the predictions to the new data\n",
    "#new_data = predictions\n",
    "#print((new_data))\n",
    "#print(scaler.inverse_transform(new_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kunstig inteligens i dag\n",
    "I dag er kunstig inteligens både utbredt, men samtidig begrenset. Det har lenge blitt brukt i industrielle tilfeller, men har i det siste tatt mer av i forbruker sektoren. Det har lenge vært mulig å få tilgang til kunstig inteligens gjennom APIer eller liknende, men det er ikke oppnårlig for den vanlige forburkeren. Med ankomsten av ChatGPT ble det mulig for den gjennomsnittlige forbrukeren å bruke AI i det daglige livet sitt. Selv om AI kan gjøre mange ting er det fortsatt ting det kan bli bedre på. GPT-3 er en modell som var trent på eldre data og var optimalisert for samtale flyt og ikke nøyaktighet. Dette gjorde at den slet med f.eks. matte og den fant opp fakta. Den var veldig selvsikker, selv når den hadde feil. Så integrerte Microsoft OpenAI sin GPT-3 modell, som ChatGPT var basert på, inn i Bing og ga den tilgang til internett. Dette løste noen av problemene med at den bare fant opp fakta og kilder, men bing chat hadde sine egene problemer. I starten var bing chat i blant veldig ekstrem, den var både aggressiv og overveldende vennlig. Microsoft måtte begrense den veldig og har sakte løsnet på begrensningene siden. Vi kan se at AI har mange problemer i utviklingsfasen og at forbrukere ikke alltid forstår disse begrensningene."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fotnoter\n",
    "<span id=\"fn1\"> 1. https://levity.ai/blog/ai-bias-how-to-avoid </span> <br>\n",
    "<span id=\"fn2\"> footnote 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
